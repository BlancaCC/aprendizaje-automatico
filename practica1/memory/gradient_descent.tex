

\section{ Gradient descent 's algorithm }
\subsection{Introduction}

Gradient descent is a general technique for minimizing a twice-differentiable functions through its slope. \cite{LFD}
It is used to find local minimums due to the facts that the start point it is crucial.

The basic idea is to update the weights using the gradients until it is not possible to continuous minimizing the error.


\medskip

\subsection{Math}

We are going to define  algorithm:

Let $w(0) \in \mathbb{R}^d$ be an arbitrary initial point,
$E : \mathbb{R}^d \times \mathbb{R}^d \longrightarrow \mathbb R$
a class $C^2$ function. The step size $\eta \in \mathbb{R}^+$
is a experimental coefficient about how much are we going to follow the slope to obtain the new weight.  
Let  $w(t) \in \mathbb{R}^d  \quad t \in \mathbb N$ be the weight for $t$ iteration which is defined as

\begin{equation*}
  w(t+1) = w(t) - \eta \nabla E_{in}(w(t))
\end{equation*}

\subsubsection{ Properties}

\begin{itemize}
\item This algorithm gives local minimums.
\item Convergence it is not assured, so it would be necessary some stop criteria. 
\item For a convex function it would be a unique global minimum.
\item $\eta$ variable in time  is important: fixes learning gradient descent algorithm. 
\end{itemize}


\subsection{Algorithm}

The following code snippet implement the algorithm, where $w(0)$ is the $initial\_point$, $E$ is the error, $\nabla E_{in}(w)$ is $gradient\_function$ and
finally $\eta$ is $eta.$ The value $\eta = 0.1$ is a heuristic basic on purely practical observation \cite{LFD}.

This algorithm does not  it is necessary to figured a stop condition, otherwise the algorithm could be

In order to avoid an infinite search, our stop criteria are a limit in the number of iteration $max\_iter$ and an error tolerance . 

\begin{minted}{python}

def gradient_descent(initial_point, E, gradient_function,  eta, max_iter, target_error):
    '''
    initicial point: w_0 
    E: error function 
    gradient_function
    eta:  step size 

    ### stop conditions ###
    max_iter
    target_error
    '''

    iterations = 0
    error = E( initial_point[0], initial_point[1])
    w = initial_point
  
    while ( (iterations < max_iter) and(error > target_error)): 

        w = w - eta * gradient_function(w[0], w[1])
        
        iterations += 1
        error = E(w[0], w[1])
 
    
    return w, iterations

\end{minted}

\subsection{Problem 1}

We want to solve the following problem: \\

Run gradient descent's algorithm to find a minimum for
function $E(u,v) = (u^3 e^{(v-s)} - 2* v^2 e^{-u})^2.$
Start with $(u,v)=(1,1)$ and $\eta = 1.0$


\subsubsection{Solve analytically the gradient of $E(u,v)$}


\begin{multline*}
  \nabla E(u,v) = \left( \frac{\partial}{\partial u}(u^3 e^{(v-2)} - 2* v^2 e^{-u})^2 , \frac{\partial}{\partial v} (u^3 e^{(v-2)} - 2 v^2 e^{-u})^2 \right) = \\
 =  \left(2(u^3 e^{(v-s)} - 2* v^2 e^{-u})(3u^2e^{(v-2)} + 2 v^2 e^{-u} ), 2(u^3 e^{(v-s)} - 2* v^2 e^{-u})(u^3 e^{(v-2)} - 4 v e^{-u}) \right)
\end{multline*}


\subsubsection{Number of iterations and final coordinates.}

Firstable we need to use 64-bits float, so we are going to use the data type $float64$ of numpy library \cite{float64}.

The functions' declaration are:

\begin{minted}{python}
  def dEu(u,v):
    '''
    Partial derivate of E with respect to the variable u
    '''
    return np.float64(
        2
        *( 3* u**2 * np.e**(v-2) + 2*v**2 * np.e**(-u) )
        *( u**3 * np.e**(v-2) - 2*v**2 * np.e**(-u))
    )
    
def dEv(u,v):
    '''
    Partial derivate of E with respect to the variable v
    '''
    return np.float64(
        2*
        ( u**3 * np.e**(v-2) - 2*v**2 * np.e**(-u) )
        *( u**3 * np.e**(v-2) - 4*v * np.e**(-u))
    )


def gradE(u,v):
    ''' 
        gradient of E
    '''
    return np.array([dEu(u,v), dEv(u,v)])

\end{minted}

To obtain the number of iterations and the final coordinates the only thing we need to do is to call $gradien\_descent$ function with the initial conditions:

\begin{minted}{python}
eta = 0.01 
max_iter = 10000000000
target_error = 1e-14
initial_point = np.array([1.0,1.0])
w, it = gradient_descent( initial_point,E, gradE, eta, max_iter, target_error )
\end{minted}

The result are:

\begin{itemize}
\item Numbers of iterations: $178.$
\item Final coordinates: $( 1.161779094157124 ,  0.9244949718723753 ).$
\end{itemize}


\subsection{Problem 2}